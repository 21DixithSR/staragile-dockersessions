Install Terraform on lab:

# sudo su -

wget -O- https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg

echo "deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main" | sudo tee /etc/apt/sources.list.d/hashicorp.list

sudo apt update && sudo apt install terraform

==================================

 Store AWS credentials in a shared credentials file and then use it in the TF config file.
In this way your accesskey and secret key will not be exposed to the outside world.


# apt-get update

# apt-get install awscli -y

# aws configure

Give the valid access key

Give the valid secret key

Press enter, no need to give any region and format option
To verify if the credentials have been set for aws

# cat ~/.aws/credentials

=====================================

Provisioners:
If we have to perform a set of actions on the local machine or on the AWS remote machine we can then use terraform provisioners

Local-exec provisioners:

Using this provisioner terraform can run a command or a script on the local machine(lab machine) where terraform is present.

Example:
If we generate TLS private key and public key
We can use local-exec provisioner to run a command that will copy the private key into a new file on the local machine.

The provisioner block is always nested inside the resource block

Demo of Local-exec provisioner:
=====================================
# mkdir provisioner-demo
# cd provisioner-demo

# vim main.tf
 
provider "aws" {

region = "us-east-1"


}

resource "tls_private_key" "mykey" {
  algorithm = "RSA"

}

resource "aws_key_pair" "aws_key" {
  key_name   = "web-key"
  public_key = tls_private_key.mykey.public_key_openssh

  provisioner "local-exec" {
  command = "echo '${tls_private_key.mykey.private_key_openssh}' > ./web-key.pem"

}

}

Save the file

# terraform init

# terraform apply

Remote-exec:
===============================

IN the same directory provisioner-demo
Create a new file
# vim remote-exec.tf

resource "aws_vpc" "sl-vpc" {
 cidr_block = "10.0.0.0/16"
  tags = {
   Name = "sl-vpc"
}

}

resource "aws_subnet" "subnet-1"{

vpc_id = aws_vpc.sl-vpc.id
cidr_block = "10.0.1.0/24"
depends_on = [aws_vpc.sl-vpc]
map_public_ip_on_launch = true
  tags = {
   Name = "sl-subnet"
}

}

resource "aws_route_table" "sl-route-table"{
vpc_id = aws_vpc.sl-vpc.id
  tags = {
   Name = "sl-route-table"
}

}

resource "aws_route_table_association" "a" {
  subnet_id      = aws_subnet.subnet-1.id
  route_table_id = aws_route_table.sl-route-table.id
}


resource "aws_internet_gateway" "gw" {
 vpc_id = aws_vpc.sl-vpc.id
 depends_on = [aws_vpc.sl-vpc]
   tags = {
   Name = "sl-gw"
}

}

resource "aws_route" "sl-route" {

route_table_id = aws_route_table.sl-route-table.id
destination_cidr_block = "0.0.0.0/0"
gateway_id = aws_internet_gateway.gw.id


}

variable "sg_ports" {
type = list(number)
default = [8080,80,22,443]

}




resource "aws_security_group" "sl-sg" {
  name        = "sg_rule"
  vpc_id = aws_vpc.sl-vpc.id
  dynamic  "ingress" {
    for_each = var.sg_ports
    iterator = port
    content{
    from_port        = port.value
    to_port          = port.value
    protocol         = "tcp"
    cidr_blocks      = ["0.0.0.0/0"]
    }
  }
egress {

    from_port        = 0
    to_port          = 0
    protocol         = "-1"
    cidr_blocks      = ["0.0.0.0/0"]


}

}
resource "aws_instance" "myec2" {
  ami           = "ami-0a9a48ce4458e384e"
  instance_type = "t2.micro"
  key_name = "web-key"
  subnet_id = aws_subnet.subnet-1.id
  security_groups = [aws_security_group.sl-sg.id]
  tags = {
    Name = "Terrafrom-EC2"
  }
  provisioner "remote-exec" {
  connection {
    type     = "ssh"
    user     = "ec2-user"
    private_key = tls_private_key.mykey.private_key_pem
    host     = self.public_ip
  }
  inline = [
  "sudo yum install httpd -y",
  "sudo systemctl start httpd",
  "sudo systemctl enable httpd",
  "sudo yum install git -y"


]

}
}


Save the file
# terraform apply


---------------------------
Manage terraform State File:
=============================

Terraform we do create or update resources -> we will find a terraform state file created or updated automatically

What ever infrastructure that terraform creates it will be recorded in the terraform state file
In which ever folder we run the terraform command → same folder terraform state file will be present

The terraform state file is created in JSON format storing all parameters of resources created in AWS

The terraform state file is a Private API
================================

The state file format is a private API that is meant to be used with in Terraform only

We should never manually update the terraform state file, it should be automatically managed by Terraform.
If we have to do any changes on the terraform state file it is recommended to use terraform state command or terraform import command

It is always better to take back of the terraform state file:
==========================================
> if we are using terraform for personal practice or use then we can maintain the state file in our local machine itself.

But if we are using terraform with multiple team members in a real time project then in that case: 
 > we will have to maintain the state file in a shared storage or location so that each team member 
 can access the same terraform state file

> But if multiple people try to run terraform command using the same state file, 
then we will get into errors related to state file locking

> Also when multiple people are using the same state file than we may have more 1 person
 updating the same infrastructure, so we will get conflicts or our file may have data loss or may be corrupted

> this can be resolved by isolating the environments for each team member and each one of them having their copy of state file

The solution could be:
==============================

The most common solution is -> to place the file in the version control tool along with the terraform code.
However this may also lead to major issues:

Manual error -> team members may pull the latest code and state file.. Make changes but may forget to push the changes to the VC tool
In VC tool when the team members pull the changes there is concept of lock in VC tool..
 Now both the team members may be running the same state file
There are chances that we may expose sensitive data via state file in VC tool

Storing State files in VC tool is not the best solution.

So some other solutions could be:

Using terraform built-in support for remote backends
Remote backends determines how terraform loads and stores the state files
We have been using this default backend in terraform so far,
 but that was local backends which has been storing your state file in local disk or directory
 
Terraform remote backends allows us to store the state file in a remote shared store. 
For example in Amazon S3, Azure storage, GCP storage, terraform cloud

Using the remote backend storage we will be able to resolve:

Manual errors: With remote backend, terraofrm will by itself update the state file on remote storage when ever you run terraform apply command
So there is no chance of manual errors.

Locking: Most of the remote backend storage supports Locking. When anybodys run terraform apply command,
 terraform will automatically acquire a lock on the state file, 
 so now if someone else runs a terraform apply, the user will have to wait as the state file is locked.
           Here, we can also add a  -local-timeout=10 mins parameter to instruct terraform to wait up to 10 mins for the local to be released and then run the next terraform apply

Secrets: Most of the backends store supports encryption in transit and encryption in the state file
        Also all the storage on remote also are secure and one should have correct permissions and roles to access the storage

We will use AMazon S3 (Simple storage service) bucket to store out state files remotely.
The reason being:

We already using this provider in our terraform configuration, so no special set up is needed
We already know that S3 bucket is designed in such a way that it is 99.999% always available, which means we dont have to worry about data loss or outages
Its supports encryption so we can easily manage sensitive data
It supports locking of state file using dynamo DB
It supports versioning of the state file, so you can always roll back to older versions if something goes down.
It is inexpensive and available in AWS for free
=======================================

Demo:

# mkdir statedemo
# cd statedemo

# vim state-demo.tf

provider "aws" {
  region = "us-east-1"
  shared_credentials_files = ["~/.aws/credentials"]

}

resource "aws_s3_bucket" "terraform-state" {
  bucket = "terraform-s3-bucket-state-file"
  lifecycle{
   prevent_destroy = true

}
}

resource "aws_s3_bucket_versioning" "versioning_example" {
  bucket = aws_s3_bucket.terraform-state.id
  versioning_configuration {
    status = "Enabled"
  }
}

resource "aws_s3_bucket_server_side_encryption_configuration" "example" {
  bucket = aws_s3_bucket.terraform-state.id

  rule {
   apply_server_side_encryption_by_default {
       sse_algorithm     = "AES256"

}
}
}


resource "aws_s3_bucket_public_access_block" "example" {
  bucket = aws_s3_bucket.terraform-state.id

  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}



#  terraform init
# terraform apply

Open the same file and add the backend information


terraform {
  backend "s3" {
    bucket = "terraform-s3-bucket-state-file"
    key    = "global/s3/terraform.tfstate"
    region = "us-east-1"
    encrypt = true
  }
}


#  terraform init

Give yes

Go to aws → search for s3 bucket -> you will see your bucket.

